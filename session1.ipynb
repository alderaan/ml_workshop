{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "session1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/alderaan/ml_workshop/blob/master/session1.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "T5NMo92vx0vg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Session 1: Escaping GridWorld with Simple RL Agents"
      ]
    },
    {
      "metadata": {
        "id": "Tq-XhIDSx0vh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Colab Setup"
      ]
    },
    {
      "metadata": {
        "id": "BJ5ogZ7Xx0vi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "3fbb6898-e6c6-443d-8d31-fe1e8ea584e0"
      },
      "cell_type": "code",
      "source": [
        "#uncomment only if you're running from google colab\n",
        "!git clone https://github.com/Datatouille/rl-workshop\n",
        "!mv rl-workshop/* .\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rl-workshop'...\n",
            "remote: Enumerating objects: 96, done.\u001b[K\n",
            "remote: Counting objects: 100% (96/96), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 96 (delta 43), reused 81 (delta 31), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (96/96), done.\n",
            "img\t README.md    sample_data     session1.ipynb  solutions\n",
            "LICENSE  rl-workshop  session0.ipynb  session2.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9mjuX1q2x0vl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "DqOJ0PSux0vm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "#cross check with our solutions once you finish\n",
        "# from solutions.agents import MCAgent\n",
        "from solutions.environments import Gridworld"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qRXJjVy-x0vo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What is Reinforcement Learning"
      ]
    },
    {
      "metadata": {
        "id": "khmlkL1Fx0vp",
        "colab_type": "code",
        "colab": {},
        "outputId": "be37e0c0-e07b-4e1c-b045-8ab2be877c7a"
      },
      "cell_type": "code",
      "source": [
        "%%HTML\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/SvXXpmXedHE\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/SvXXpmXedHE\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "DJ0C_T-Nx0vs",
        "colab_type": "code",
        "colab": {},
        "outputId": "07f11e1d-8ee1-44f4-aff0-12a3b8bda9b6"
      },
      "cell_type": "code",
      "source": [
        "%%HTML\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ru5xQPZPRUI\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ru5xQPZPRUI\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "FnojpQ9Xx0vw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## How Useful is Reinforcement Learning"
      ]
    },
    {
      "metadata": {
        "id": "dm7lLGikx0vy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Deep reinforcement learning has been successful (better than or equal to supervised learning) in:\n",
        "* Beating professional [Go players](https://deepmind.com/research/alphago/), [Flappy Bird](https://www.youtube.com/watch?v=hri7ir5qhj0) and, to some extent [DotA players](https://youtu.be/UZHTNBMAfAA?t=2m40s).\n",
        "* Reduce data center power usage and [save 40% of electricity bill](https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/)\n",
        "* Neural architecture search for better algorithms aka [AutoML](https://cloud.google.com/automl/)\n",
        "* Control tasks for robotics like [this](https://www.youtube.com/watch?v=W_gxLKSsSIE) and [this](https://www.youtube.com/watch?v=gn4nRCC9TwQ)\n",
        "* [Self-driving cars](https://www.youtube.com/watch?v=opsmd5yuBF0)\n",
        "* [Ads bidding](http://wnzhang.net/papers/rlb.pdf)\n",
        "\n",
        "And most likely being tried for:\n",
        "* [Algorithmic trading](http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/)\n",
        "* [Supply chain management](https://www.youtube.com/watch?v=gQa6iWGcGWY)\n",
        "* Drug discovery\n",
        "* Security\n",
        "* Recommendation engines"
      ]
    },
    {
      "metadata": {
        "id": "klL9ecbEx0vy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Why is Reinforcement Learning Different"
      ]
    },
    {
      "metadata": {
        "id": "tPOamMUOx0vz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Level of workflow automation in classes of machine learning algorithm:\n",
        "* Supervised Learning\n",
        "$$\\text{Raw Data (X,y)} \\Rightarrow \\text{Labeled Data} \\rightarrow \\text{Model} \\rightarrow \\text{Predicted Labels} \\Rightarrow \\text{Actions}$$\n",
        "\n",
        "* Unsupervised Learning\n",
        "$$\\text{Raw Data (X)} \\rightarrow \\text{Model} \\rightarrow \\text{Predicted Clusters} \\Rightarrow \\text{Labled Clusters} \\Rightarrow \\text{Actions}$$\n",
        "\n",
        "* Reinforcement Learning\n",
        "$$\\text{Raw Data} \\Rightarrow \\text{RL Scheme (S,A,R,S',A')} \\rightarrow \\text{Model} \\rightarrow \\text{Action}$$\n",
        "\n",
        "\n",
        "where $\\Rightarrow$ denotes transformations that require humans and $\\rightarrow$ denotes those that do not."
      ]
    },
    {
      "metadata": {
        "id": "ZAfE96Cwx0vz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Fill in The Code"
      ]
    },
    {
      "metadata": {
        "id": "YqK0YbAbx0v0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import sys\n",
        "\n",
        "\"\"\"\n",
        "Coding assignment order:\n",
        "1. select_action\n",
        "2. get_v\n",
        "\"\"\"\n",
        "\n",
        "class MCAgent:\n",
        "    def __init__(self, env, policy, gamma = 0.9, \n",
        "                 start_epsilon = 0.9, end_epsilon = 0.1, epsilon_decay = 0.9):\n",
        "        self.env = env\n",
        "        self.n_action = len(self.env.action_space)\n",
        "        self.policy = policy\n",
        "        self.gamma = gamma\n",
        "        self.v = dict.fromkeys(self.env.state_space,0)\n",
        "        self.n_v = dict.fromkeys(self.env.state_space,0)\n",
        "        self.q = defaultdict(lambda: np.zeros(self.n_action))\n",
        "        self.n_q = defaultdict(lambda: np.zeros(self.n_action))\n",
        "        self.start_epsilon = start_epsilon\n",
        "        self.end_epsilon = end_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "    def get_epsilon(self,n_episode):\n",
        "        epsilon = max(self.start_epsilon * (self.epsilon_decay**n_episode),self.end_epsilon)\n",
        "        return(epsilon)\n",
        "    def get_v(self,start_state,epsilon = 0.):\n",
        "        episode = self.run_episode(start_state,epsilon)\n",
        "        \"\"\"\n",
        "        Write the code to calculate the state value function of a state \n",
        "        given a deterministic policy.\n",
        "        \"\"\"\n",
        "        episode = self.run_episode(start_state,epsilon)\n",
        "        v=0\n",
        "        return(v)\n",
        "    def get_q(self, start_state, first_action, epsilon=0.):\n",
        "        episode = self.run_episode(start_state,epsilon,first_action)\n",
        "        q = np.sum([episode[i][2] * self.gamma**i for i in range(len(episode))])\n",
        "        return(q)\n",
        "    def select_action(self,state,epsilon):\n",
        "        \"\"\"\n",
        "        Currently the agent only selects a random action.\n",
        "        Write the code to make the agent perform \n",
        "        according to an epsilon-greedy policy.\n",
        "        \"\"\"\n",
        "        \n",
        "        best_action = self.policy[state]\n",
        "        if random.random() > epsilon:\n",
        "            action = best_action\n",
        "        else:\n",
        "            action = np.random.choice(np.arange(self.n_action))\n",
        "        return(action)\n",
        "      \n",
        "    def print_policy(self):\n",
        "        for i in range(self.env.sz[0]):\n",
        "            print('\\n----------')\n",
        "            for j in range(self.env.sz[1]):\n",
        "                p=self.policy[(i,j)]\n",
        "                out = self.env.action_text[p]\n",
        "                print(f'{out} |',end='')\n",
        "    def print_v(self, decimal = 1):\n",
        "        for i in range(self.env.sz[0]):\n",
        "            print('\\n---------------')\n",
        "            for j in range(self.env.sz[1]):\n",
        "                out=np.round(self.v[(i,j)],decimal)\n",
        "                print(f'{out} |',end='')\n",
        "    def run_episode(self, start, epsilon, first_action = None):\n",
        "        result = []\n",
        "        state = self.env.reset(start)\n",
        "        #dictate first action to iterate q\n",
        "        if first_action is not None:\n",
        "            action = first_action\n",
        "            next_state,reward,done = self.env.step(action)\n",
        "            result.append((state,action,reward,next_state,done))\n",
        "            state = next_state\n",
        "            if done: return(result)\n",
        "        while True:\n",
        "            action = self.select_action(state,epsilon)\n",
        "            next_state,reward,done = self.env.step(action)\n",
        "            result.append((state,action,reward,next_state,done))\n",
        "            state = next_state\n",
        "            if done: break\n",
        "        return(result)\n",
        "    def update_policy_q(self):\n",
        "        for state in self.env.state_space:\n",
        "            self.policy[state] = np.argmax(self.q[state])\n",
        "    def mc_predict_v(self,n_episode=10000,first_visit=True):\n",
        "        for t in range(n_episode):\n",
        "            traversed = []\n",
        "            e = self.get_epsilon(t)\n",
        "            transitions = self.run_episode(self.env.start,e)\n",
        "            states,actions,rewards,next_states,dones = zip(*transitions)\n",
        "            for i in range(len(transitions)):\n",
        "                if first_visit and (states[i] not in traversed):\n",
        "                    traversed.append(states[i])\n",
        "                    self.n_v[states[i]]+=1\n",
        "                    discounts = np.array([self.gamma**j for j in range(len(transitions)+1)])\n",
        "                    self.v[states[i]]+= sum(rewards[i:]*discounts[:-(1+i)])\n",
        "        for state in self.env.state_space:\n",
        "            if state != self.env.goal:\n",
        "                self.v[state] = self.v[state] / self.n_v[state]\n",
        "            else:\n",
        "                self.v[state] = 0\n",
        "    \n",
        "    def mc_predict_q(self,n_episode=10000,first_visit=True):\n",
        "        for t in range(n_episode):\n",
        "            traversed = []\n",
        "            e = self.get_epsilon(t)\n",
        "            transitions = self.run_episode(self.env.start,e)\n",
        "            states,actions,rewards,next_states,dones = zip(*transitions)\n",
        "            for i in range(len(transitions)):\n",
        "                if first_visit and ((states[i],actions[i]) not in traversed):\n",
        "                    traversed.append((states[i],actions[i]))\n",
        "                    self.n_q[states[i]][actions[i]]+=1\n",
        "                    discounts = np.array([self.gamma**j for j in range(len(transitions)+1)])\n",
        "                    self.q[states[i]][actions[i]]+= sum(rewards[i:]*discounts[:-(1+i)])\n",
        "                elif not first_visit:\n",
        "                    self.n_q[states[i]][actions[i]]+=1\n",
        "                    discounts = np.array([self.gamma**j for j in range(len(transitions)+1)])\n",
        "                    self.q[states[i]][actions[i]]+= sum(rewards[i:]*discounts[:-(1+i)])\n",
        "\n",
        "        #print(self.q,self.n_q)\n",
        "        for state in self.env.state_space:\n",
        "            for action in range(self.n_action):\n",
        "                if state != self.env.goal:\n",
        "                    self.q[state][action] = self.q[state][action] / self.n_q[state][action]\n",
        "                else:\n",
        "                    self.q[state][action] = 0\n",
        "        \n",
        "    def mc_control_q(self,n_episode=10000,first_visit=True):\n",
        "        self.mc_predict_q(n_episode,first_visit)\n",
        "        self.update_policy_q()\n",
        "        \n",
        "    def mc_control_glie(self,n_episode=10000,first_visit=True,lr=0.):\n",
        "        for t in range(n_episode):\n",
        "            traversed = []\n",
        "            e = self.get_epsilon(t)\n",
        "            transitions = self.run_episode(self.env.start,e)\n",
        "            states,actions,rewards,next_states,dones = zip(*transitions)\n",
        "            for i in range(len(transitions)):\n",
        "                if first_visit and ((states[i],actions[i]) not in traversed):\n",
        "                    traversed.append((states[i],actions[i]))\n",
        "                    self.n_q[states[i]][actions[i]]+=1\n",
        "                    discounts = np.array([self.gamma**j for j in range(len(transitions)+1)])\n",
        "                    g = sum(rewards[i:]*discounts[:-(1+i)])\n",
        "                    if lr > 0:\n",
        "                        a = lr\n",
        "                    else:\n",
        "                        a = (1/self.n_q[states[i]][actions[i]])\n",
        "                    self.q[states[i]][actions[i]]+= a*(g - self.q[states[i]][actions[i]])\n",
        "                    self.update_policy_q()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NQwwNXi-x0v2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reinforcement Learning Framework"
      ]
    },
    {
      "metadata": {
        "id": "ilATa_6Gx0v2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Reinforcement learning formulates interaction between an **agent** and its **environment** as **Markov decision processes**. For a given **state**, an agent takes an **action** based on the current **state**. In response to that action at that state, the agent will then get some **reward** from the environment, and that state changes to the next one.\n",
        "\n",
        "$S_0 \\rightarrow A_0 \\rightarrow R_1 \\rightarrow S_1 \\rightarrow A_1 \\rightarrow R_2 \\rightarrow S_2 \\rightarrow ... \\rightarrow S_{t-1} \\rightarrow A_{t-1} \\rightarrow R_t \\rightarrow S_{t}$\n",
        "\n",
        "where $t$ is the last time step and $S_t$ is the **terminal state** meaning an **episode** of the interactions ended. RL problems that have an end are called **episodic tasks** and those that do not are called **continuous tasks**."
      ]
    },
    {
      "metadata": {
        "id": "BE0nyXRyx0v3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![RL Framework](https://github.com/Datatouille/rl-workshop/blob/master/img/rl_framework.png?raw=1)\n",
        "Source: [Sutton and Barto](https://cdn.preterhuman.net/texts/science_and_technology/artificial_intelligence/Reinforcement%20Learning%20%20An%20Introduction%20-%20Richard%20S.%20Sutton%20,%20Andrew%20G.%20Barto.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "5z7zEup_x0v4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Example\n",
        "\n",
        "* Walking robots\n",
        "    * Environment: sidewalks\n",
        "    * Agent: a robot\n",
        "    * States: positions, velocities and accelerations of body parts\n",
        "    * Actions: move arms, legs, various joints in the body\n",
        "    * Rewards: fall or not\n",
        "    * Episodes: seconds, miliseconds until fall\n",
        "* Ads bidding\n",
        "    * Environment: Google Adwords\n",
        "    * Agent: an ecommerce company\n",
        "    * States: campaign impressions, clicks, purchases\n",
        "    * Actions: adjust bid prices and budget\n",
        "    * Rewards: conversion rates, cost of new customer acquisitions\n",
        "    * Episodes: daily or hourly until campaign ends\n",
        "* Retail stock trading\n",
        "    * Environment: the stock market\n",
        "    * Agent: a retail investor who cannot influence market prices\n",
        "    * States: market prices, volumes, and other indicators\n",
        "    * Actions: buy, hold, sell\n",
        "    * Rewards: returns, returns adjusted by volatility, and so on\n",
        "    * Episodes: daily, hourly, every second until we are extremely rich or broke\n",
        "    \n",
        "**Concept Assigment** Come up with one or more scenarios that you think could be framed as a reinforcement learning problem and list their elements like the example above. Some candidates are:\n",
        "* [AlphaGo](https://deepmind.com/research/alphago/)\n",
        "* [OpenAI Five Dota Bots](https://openai.com/five/)\n",
        "* [Pancake-flipping Robots](https://www.youtube.com/watch?v=W_gxLKSsSIE&list=PL5nBAYUyJTrM48dViibyi68urttMlUv7e)"
      ]
    },
    {
      "metadata": {
        "id": "rO39Direx0v5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Markov Property - One-step Dynamics"
      ]
    },
    {
      "metadata": {
        "id": "q8Fg0IzDx0v7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One of the most fundamental assumptions of many reinforcement learning algorithms is that the interactions of the world has **Markov property**. That is, the transitions between states only depend on the current state and action, and not the history of transitions formulated as:\n",
        "\n",
        "$$P(s_{t+1},r_{t+1}|s_t,a_t,r_t,s_{t-1},a_{t-1},...,r_1,s_0,a_0) = P(s_{t+1},r_{t+1}|s_t,a_t)$$\n",
        "\n",
        "This can be seen as unrealistic as we intuitively use cues from a history of transitions to choose our actions in real life but as you will learn its simplicity has merits in terms of reduced complexity in real-world applications and we can incorporate long-term goals into reinforcement learning. In fact, long-term goals are one of the most important research area in reinforcement learning today.\n",
        "\n",
        "![RL Framework](https://github.com/Datatouille/rl-workshop/blob/master/img/mdp_weather.png?raw=1)\n",
        "\n",
        "**Quick Notation** $P(x|y)$ means the probability of $x$ occuring given $y$. For instance, $P(cold|cough)$ is probability that you have a cold, given that you are coughing.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "A4uUBldqx0v7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Environment - Gridworld"
      ]
    },
    {
      "metadata": {
        "id": "g0b2HnHnx0v8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### What The Gridworld Looks Like"
      ]
    },
    {
      "metadata": {
        "id": "aa5ZHX9dx0v9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are going to work on an equivalent of a poor man's version of [Unity's Gridworld](http://awjuliani.github.io/GridGL/).\n",
        "\n",
        "* `F` - Free tiles; agent can move to\n",
        "* `T` - Traps; agent recieves some penalty\n",
        "* `G` - The Goal, episode ends once reached"
      ]
    },
    {
      "metadata": {
        "id": "wRfVSaTDx0v-",
        "colab_type": "code",
        "colab": {},
        "outputId": "2fd765cc-94e6-4f47-fb85-3bea5fb6e0b4"
      },
      "cell_type": "code",
      "source": [
        "env = Gridworld()\n",
        "env.print_physical(visible_only=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------------\n",
            "['F', 'x'] |['F', 'x'] |['F', 'x'] |\n",
            "------------------------------------\n",
            "['F', 'x'] |['T', 'x'] |['G', 'x'] |\n",
            "------------------------------------\n",
            "['F', 'o'] |['F', 'x'] |['F', 'x'] |"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "74nUotTkx0wC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### States"
      ]
    },
    {
      "metadata": {
        "id": "nnydFV49x0wC",
        "colab_type": "code",
        "colab": {},
        "outputId": "ee34ee6a-1d7a-44d1-f16c-d74b6b180fb6"
      },
      "cell_type": "code",
      "source": [
        "env.state_space"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "Jv8UI3tIx0wF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Actions"
      ]
    },
    {
      "metadata": {
        "id": "N83nIog7x0wF",
        "colab_type": "code",
        "colab": {},
        "outputId": "10f65701-5cf4-4cfc-8ed8-07d157d2c3a7"
      },
      "cell_type": "code",
      "source": [
        "env.action_space, env.action_text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1, 2, 3]), array(['U', 'L', 'D', 'R'], dtype='<U1'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "bE4nwBdZx0wI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Rewards"
      ]
    },
    {
      "metadata": {
        "id": "SJQ1hX8yx0wJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "b3829625-4963-4773-936a-058a3a51603b"
      },
      "cell_type": "code",
      "source": [
        "env.print_reward()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------\n",
            "0 |0 |0 |\n",
            "----------\n",
            "0 |-5 |5 |\n",
            "----------\n",
            "0 |0 |0 |"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_4I4GYJ7x0wL",
        "colab_type": "code",
        "colab": {},
        "outputId": "1676c8c0-82cc-4930-bef6-f4ae9cb9b21f"
      },
      "cell_type": "code",
      "source": [
        "env.move_reward"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "QqIYdfZRx0wP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### What The Agent Usually Sees"
      ]
    },
    {
      "metadata": {
        "id": "D2lox4h_x0wQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "4ed61e9c-55b5-4def-fbc1-37be0364fdcc"
      },
      "cell_type": "code",
      "source": [
        "env.print_physical(visible_only=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------------\n",
            "['NA', 'NA'] |['NA', 'NA'] |['NA', 'NA'] |\n",
            "------------------------------------\n",
            "['NA', 'NA'] |['NA', 'NA'] |['NA', 'NA'] |\n",
            "------------------------------------\n",
            "['F', 'o'] |['NA', 'NA'] |['NA', 'NA'] |"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IRVTpJ4Nx0wT",
        "colab_type": "code",
        "colab": {},
        "outputId": "94986015-98e8-4a74-e94b-42b05cddcda8"
      },
      "cell_type": "code",
      "source": [
        "#go down\n",
        "action = np.argwhere(env.action_text=='U')\n",
        "print(env.step(action))\n",
        "#then right\n",
        "action = np.argwhere(env.action_text=='R')\n",
        "print(env.step(action))\n",
        "env.print_physical(visible_only=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((1, 0), -1, False)\n",
            "((1, 1), -6, False)\n",
            "\n",
            "------------------------------------\n",
            "['NA', 'NA'] |['NA', 'NA'] |['NA', 'NA'] |\n",
            "------------------------------------\n",
            "['F', 'x'] |['T', 'o'] |['NA', 'NA'] |\n",
            "------------------------------------\n",
            "['F', 'x'] |['NA', 'NA'] |['NA', 'NA'] |"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tSXhc3lSx0wV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Deterministic Environment vs Stochastic Environment"
      ]
    },
    {
      "metadata": {
        "id": "fQ2cTYzGx0wW",
        "colab_type": "code",
        "colab": {},
        "outputId": "78f2cadd-6e98-43ce-847a-641b507e9a39"
      },
      "cell_type": "code",
      "source": [
        "env = Gridworld(wind_p=0.5)\n",
        "#go down\n",
        "action = np.argwhere(env.action_text=='U')\n",
        "print(env.step(action))\n",
        "#then right\n",
        "action = np.argwhere(env.action_text=='R')\n",
        "print(env.step(action))\n",
        "env.print_physical(visible_only=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((1, 0), -1, False)\n",
            "((2, 1), -1, False)\n",
            "\n",
            "------------------------------------\n",
            "['NA', 'NA'] |['NA', 'NA'] |['NA', 'NA'] |\n",
            "------------------------------------\n",
            "['F', 'x'] |['NA', 'NA'] |['NA', 'NA'] |\n",
            "------------------------------------\n",
            "['F', 'x'] |['F', 'o'] |['NA', 'NA'] |"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g4MTnVWox0wa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Agent"
      ]
    },
    {
      "metadata": {
        "id": "o4mnkC9px0wa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Deterministic Policy"
      ]
    },
    {
      "metadata": {
        "id": "iR7fHKlgx0wc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\pi(s) \\in \\mathcal{A}(s)$ for all $s \\in \\mathcal{S}$"
      ]
    },
    {
      "metadata": {
        "id": "Z2dQzD-3x0wc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "819c5aa7-ef06-48dd-f5e2-8afc65612edb"
      },
      "cell_type": "code",
      "source": [
        "#deterministic env\n",
        "env = Gridworld(wind_p=0.)\n",
        "#deterministic policy\n",
        "policy_a = {(0, 0): 3,\n",
        "          (0, 1): 3,\n",
        "          (0, 2): 2,\n",
        "          (1, 0): 3,\n",
        "          (1, 1): 3,\n",
        "          (1, 2): 0,\n",
        "          (2, 0): 3,\n",
        "          (2, 1): 0,\n",
        "          (2, 2): 0}\n",
        "policy_b = {(0, 0): 3,\n",
        "          (0, 1): 3,\n",
        "          (0, 2): 2,\n",
        "          (1, 0): 0,\n",
        "          (1, 1): 3,\n",
        "          (1, 2): 0,\n",
        "          (2, 0): 3,\n",
        "          (2, 1): 3,\n",
        "          (2, 2): 0}\n",
        "\n",
        "#peek\n",
        "print('Reward Grid')\n",
        "env.print_reward()\n",
        "print('\\n')\n",
        "a = MCAgent(env,policy_a)\n",
        "print('Policy A: Reach Goal ASAP')\n",
        "a.print_policy()\n",
        "print('\\n')\n",
        "print('Policy B: Avoid Trap')\n",
        "a.policy = policy_b\n",
        "a.print_policy()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reward Grid\n",
            "\n",
            "----------\n",
            "0 |0 |0 |\n",
            "----------\n",
            "0 |-5 |5 |\n",
            "----------\n",
            "0 |0 |0 |\n",
            "\n",
            "Policy A: Reach Goal ASAP\n",
            "\n",
            "----------\n",
            "R |R |D |\n",
            "----------\n",
            "R |R |U |\n",
            "----------\n",
            "R |U |U |\n",
            "\n",
            "Policy B: Avoid Trap\n",
            "\n",
            "----------\n",
            "R |R |D |\n",
            "----------\n",
            "U |R |U |\n",
            "----------\n",
            "R |R |U |"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8IP7VisTx0wh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Stochastic Policy - Epsilon Greedy"
      ]
    },
    {
      "metadata": {
        "id": "Ovi02lAVx0wi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\pi(a|s) = \\mathbb{P}(A_t=a|S_t=s)$ for all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}(s)$"
      ]
    },
    {
      "metadata": {
        "id": "v-W34AK8x0wi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Coding Assignment** Implement epsilon greedy in the `select_action` function of `agent.py`.\n",
        "\n",
        "Hint: Pseudo-code looks like this\n",
        "```\n",
        "r = random number between 0 and 1\n",
        "if r > epsilon:\n",
        "    perform best action\n",
        "else:\n",
        "    perform random action\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "A06r8e-qx0wj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "5266ec1a-f356-4b2a-8590-12afefdf7eba"
      },
      "cell_type": "code",
      "source": [
        "state = (0,0)\n",
        "print(f'Preferred action for {state} is {a.policy[state]}')\n",
        "print('At epsilon 0.5, we get:')\n",
        "actions = np.array([a.select_action(state,epsilon=0.5) for i in range(10000)])\n",
        "plt.hist(actions)[2]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preferred action for (0, 0) is 3\n",
            "At epsilon 0.5, we get:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<a list of 10 Patch objects>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFKCAYAAADScRzUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noQYAwLD/B0jtGw+XpaC/AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fccdf3b0fd0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "YePlbZowx0wm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#epsilon decaying \n",
        "a = MCAgent(env,policy_a,start_epsilon=0.9,end_epsilon=0.1,epsilon_decay=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ob64BObnx0wp",
        "colab_type": "code",
        "colab": {},
        "outputId": "2e869ec9-d7cb-4f65-cbbb-9b3e7cbd36bc"
      },
      "cell_type": "code",
      "source": [
        "eps = np.array([a.get_epsilon(i) for i in range(100)])\n",
        "plt.plot(eps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f19868f1208>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "mAgGf9z_x0wu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### State Value"
      ]
    },
    {
      "metadata": {
        "id": "zCrxmTjXx0wv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$v_\\pi(s) = \\sum_{a \\in \\mathcal{A}(s)}\\pi(a|s)\\sum_{s' \\in \\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s,a)(r + \\gamma v_\\pi(s'))$"
      ]
    },
    {
      "metadata": {
        "id": "toKw2Nu_x0wv",
        "colab_type": "code",
        "colab": {},
        "outputId": "1cd0808c-0100-47c9-d03b-8a751cf90c34"
      },
      "cell_type": "code",
      "source": [
        "#deterministic env\n",
        "env = Gridworld(wind_p=0.)\n",
        "policy = {(0, 0): 3,\n",
        "          (0, 1): 3,\n",
        "          (0, 2): 2,\n",
        "          (1, 0): 3,\n",
        "          (1, 1): 3,\n",
        "          (1, 2): 0,\n",
        "          (2, 0): 3,\n",
        "          (2, 1): 0,\n",
        "          (2, 2): 0}\n",
        "a = MCAgent(env,policy = policy, gamma = 1)\n",
        "print('Reward Grid')\n",
        "env.print_reward()\n",
        "print('\\n')\n",
        "print('Policy: Reach Goal ASAP')\n",
        "a.print_policy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reward Grid\n",
            "\n",
            "----------\n",
            "0 |0 |0 |\n",
            "----------\n",
            "0 |-5 |5 |\n",
            "----------\n",
            "0 |0 |0 |\n",
            "\n",
            "Policy: Reach Goal ASAP\n",
            "\n",
            "----------\n",
            "R |R |D |\n",
            "----------\n",
            "R |R |U |\n",
            "----------\n",
            "R |U |U |"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cp2FMht9x0wx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Coding Assigment** Implement `get_v` to find out the state value of each state.\n",
        "\n",
        "Hint: Pseudo-code looks like this\n",
        "\n",
        "```\n",
        "Run through the gridworld, starting at `start_state` and save transitions to `episode`\n",
        "`episode` contains n transitions and each looks like this (state,action,reward,next_state,done)\n",
        "\n",
        "value = 0\n",
        "for i in 0 to n:\n",
        "    value += reward[i] * (gamma**i)\n",
        "return(value)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "0Qddt1ZXx0wx",
        "colab_type": "code",
        "colab": {},
        "outputId": "23f1f991-55c8-40ae-b414-c63e7e608b20"
      },
      "cell_type": "code",
      "source": [
        "for state in env.state_space:\n",
        "    a.v[state] = a.get_v(start_state=state, epsilon=0.)\n",
        "a.print_v()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---------------\n",
            "2 |3 |4 |\n",
            "---------------\n",
            "-2 |4 |3 |\n",
            "---------------\n",
            "-3 |-2 |4 |"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GmHSOYjLx0w0",
        "colab_type": "code",
        "colab": {},
        "outputId": "ae659f11-6b62-4fb7-bbcc-c838801b0cb7"
      },
      "cell_type": "code",
      "source": [
        "a.gamma = 0.5\n",
        "for state in env.state_space:\n",
        "    a.v[state] = a.get_v(start_state=state, epsilon=0.)\n",
        "a.print_v()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---------------\n",
            "-0.5 |1.0 |4.0 |\n",
            "---------------\n",
            "-4.0 |4.0 |1.0 |\n",
            "---------------\n",
            "-3.0 |-4.0 |4.0 |"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e51FW2ndx0w1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Concept assigment** What are the best discount factor $\\gamma$ for each of the following environments?\n",
        "* [CartPole OpenAI](https://github.com/openai/gym/wiki/CartPole-v0) - An agent is a cart trying to balance a pole by going left or right. It gets +1 for each step the pole stays on and +0 when the pole falls over.\n",
        "* [CartPole Alternative](http://incompleteideas.net/book/the-book-2nd.html) - An agent is a cart trying to balance a pole by going left or right. It gets -1 if the pole falls over and +0 otherwise.\n",
        "* [Banana Collector](https://www.youtube.com/watch?v=heVMs3t9qSk) - An agent is a robot who collects bananas in a room. It gets +1 for yellow bananas and -1 for blue bananas. The time limit is 300 steps."
      ]
    },
    {
      "metadata": {
        "id": "pxTJx3Tdx0w2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (State-)Action Value"
      ]
    },
    {
      "metadata": {
        "id": "hAZo-GSFx0w3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$q_\\pi(s,a) = \\sum_{s' \\in \\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s,a)(r + \\gamma\\sum_{a' \\in \\mathcal{A}(s')} \\pi(a'|s') q_\\pi(s',a'))$"
      ]
    },
    {
      "metadata": {
        "id": "rqv5M8pnx0w3",
        "colab_type": "code",
        "colab": {},
        "outputId": "d33f8d6f-4a59-4c59-e8a5-c1ee3ceb00d1"
      },
      "cell_type": "code",
      "source": [
        "print('Reward Grid')\n",
        "env.print_reward()\n",
        "print('\\n')\n",
        "print('Policy: Reach Goal ASAP')\n",
        "a.print_policy()\n",
        "\n",
        "a.gamma=1\n",
        "for state in env.state_space:\n",
        "    for action in env.action_space:\n",
        "        a.q[state][action] = a.get_q(state,action,epsilon=0.)\n",
        "\n",
        "print(f'\\nActions: {env.action_text}')\n",
        "for i in a.q: print(i,a.q[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reward Grid\n",
            "\n",
            "----------\n",
            "0 |0 |0 |\n",
            "----------\n",
            "0 |-5 |5 |\n",
            "----------\n",
            "0 |0 |0 |\n",
            "\n",
            "Policy: Reach Goal ASAP\n",
            "\n",
            "----------\n",
            "R |R |D |\n",
            "----------\n",
            "R |R |U |\n",
            "----------\n",
            "R |U |U |\n",
            "Actions: ['U' 'L' 'D' 'R']\n",
            "(0, 0) [ 1.  1. -3.  2.]\n",
            "(0, 1) [ 2.  1. -2.  3.]\n",
            "(0, 2) [3. 2. 4. 3.]\n",
            "(1, 0) [ 1. -3. -4. -2.]\n",
            "(1, 1) [ 2. -3. -3.  4.]\n",
            "(1, 2) [ 3. -2.  3. -1.]\n",
            "(2, 0) [-3. -4. -4. -3.]\n",
            "(2, 1) [-2. -4. -3.  3.]\n",
            "(2, 2) [ 4. -3.  3.  3.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M1PB_w1Lx0w6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Comparing Policies"
      ]
    },
    {
      "metadata": {
        "id": "r3wWKMZGx0w7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When comparing two policies a and b:\n",
        "\n",
        "$\\pi_a > \\pi_b$ if and only if $v_{\\pi_a}(s) > v_{\\pi_b}(s)$ for all $s \\in \\mathcal{S}$\n",
        "\n",
        "An optimal policy $\\pi^*$ is defined as:\n",
        "\n",
        "$\\pi^* > \\pi$ for all $\\pi$\n",
        "\n",
        "We can also find an optimal policy from action value function as choosing the action with the highest q-value out of any action in that state:\n",
        "\n",
        "$\\pi^*(s) = argmax_{a \\in \\mathcal{A}}q^*(s,a)$"
      ]
    },
    {
      "metadata": {
        "id": "QwoEUzR_x0w7",
        "colab_type": "code",
        "colab": {},
        "outputId": "8181dcf1-b733-4a7e-8a92-aa10f6775688"
      },
      "cell_type": "code",
      "source": [
        "#deterministic env\n",
        "env = Gridworld(wind_p=0.)\n",
        "#deterministic policy\n",
        "policy_a = {(0, 0): 3,\n",
        "          (0, 1): 3,\n",
        "          (0, 2): 2,\n",
        "          (1, 0): 3,\n",
        "          (1, 1): 3,\n",
        "          (1, 2): 0,\n",
        "          (2, 0): 3,\n",
        "          (2, 1): 0,\n",
        "          (2, 2): 0}\n",
        "policy_b = {(0, 0): 3,\n",
        "          (0, 1): 3,\n",
        "          (0, 2): 2,\n",
        "          (1, 0): 0,\n",
        "          (1, 1): 3,\n",
        "          (1, 2): 0,\n",
        "          (2, 0): 3,\n",
        "          (2, 1): 3,\n",
        "          (2, 2): 0}\n",
        "\n",
        "a = MCAgent(env,policy_a)\n",
        "print('Policy A: Reach Goal ASAP')\n",
        "a.print_policy()\n",
        "for state in env.state_space:\n",
        "    a.v[state] = a.get_v(start_state=state, epsilon=0.)\n",
        "a.print_v()\n",
        "print('\\n')\n",
        "print('Policy B: Avoid Trap')\n",
        "a.policy = policy_b\n",
        "a.print_policy()\n",
        "for state in env.state_space:\n",
        "    a.v[state] = a.get_v(start_state=state, epsilon=0.)\n",
        "a.print_v()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy A: Reach Goal ASAP\n",
            "\n",
            "----------\n",
            "R |R |D |\n",
            "----------\n",
            "R |R |U |\n",
            "----------\n",
            "R |U |U |\n",
            "---------------\n",
            "1.3 |2.6 |4.0 |\n",
            "---------------\n",
            "-2.4 |4.0 |2.6 |\n",
            "---------------\n",
            "-3.2 |-2.4 |4.0 |\n",
            "\n",
            "Policy B: Avoid Trap\n",
            "\n",
            "----------\n",
            "R |R |D |\n",
            "----------\n",
            "U |R |U |\n",
            "----------\n",
            "R |R |U |\n",
            "---------------\n",
            "1.3 |2.6 |4.0 |\n",
            "---------------\n",
            "0.2 |4.0 |2.6 |\n",
            "---------------\n",
            "1.3 |2.6 |4.0 |"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zpFuZbhdx0w-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Words of Caution\n",
        "\n",
        "A few reasons [Deep Reinforcement Learning Doesn't Work Yet](https://www.alexirpan.com/2018/02/14/rl-hard.html):\n",
        "* It still takes too much time to learn simple things; for instance, one of the previous state-of-the-art Rainbow model takes over 80 hours to learn to play Atari games at human level. Even a child would only need a few hours on their phone.\n",
        "* Supervised learning works so well.\n",
        "* Instead of labels, you decide on a reward to motivate the models and it is a tricky business; for example, how would give rewards to an humanoid robot agent for it to learn how to walk?\n",
        "* There are so many hyperparameters to take care of.\n",
        "* A lot of times it is [just random search](https://arxiv.org/abs/1803.07055).\n",
        "\n",
        "![Foundational Flaw](https://github.com/Datatouille/rl-workshop/blob/master/img/foundational_flaw.PNG?raw=1)\n",
        "\n",
        "Source: [Reinforcement learnings foundational flaw](https://thegradient.pub/why-rl-is-flawed/)"
      ]
    },
    {
      "metadata": {
        "id": "tDeLKiIvx0w_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Challenges"
      ]
    },
    {
      "metadata": {
        "id": "e2ZoRYlux0xA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Read up on Bellman's equations and find out where they hid in our workshop today.\n",
        "* What are you ideas about how we can find the policy policy?\n",
        "* Play around with Gridworld. Tweak these variables and see what happens:\n",
        "    * Expand the grid and/or add some more traps\n",
        "    * Wing probability\n",
        "    * Move rewards\n",
        "    * Discount factor\n",
        "    * Epsilon and how to decay it (or not)"
      ]
    },
    {
      "metadata": {
        "id": "cEUN4UCmx0xA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}